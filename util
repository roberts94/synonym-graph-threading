# -*- coding: utf-8 -*-
"""
@author: Ross Roberts
"""
import re
import random
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import wordnet as wn

pos = ['n', 'v', 'a', 's', 'r']


def get_graph(csv):
    graph = pd.read_csv(csv)
    v = {}
    for word in graph.v.dropna().tolist():
        v[word] = set([ss.pos() for ss in wn.synsets(word)])
        if 's' in v[word]:
            v[word].add('a')
    e = [pair.split(',') for pair in graph.e.dropna().tolist()]
    return  v, e


def draw_graph(edges):
    df = pd.DataFrame({'from':[pair[0] for pair in edges],
                       'to':[pair[1] for pair in edges]})
    G = nx.from_pandas_edgelist(df, 'from', 'to')
    nx.draw(G, 
            with_labels=True, 
            font_size=20,
            font_color="black",
            node_size=2000,
            node_color="skyblue",
            alpha=0.5,
            node_shape="o",
            edge_color="black",
            width=0.5,
            pos=nx.circle_layout(G))
    plt.show()
    

def connected(prev, edges):
    rs = set()
    for pair in edges:
        if prev in pair:
            rs.add(pair[(pair.index(prev)+1)%2])
    return rs


def random_path(start, edges, length, num_tries):
    count1 = 0
    rl = [start]
    while len(rl) < length and count1 < num_tries:
        rl = [start]
        rs = {start}
        prev = start
        end = False
        count2 = 0
        while not end and count2 < length-1:
            available = connected(prev, edges) - rs    
            if available:
                to_add = random.sample(available, 1)[0]
                rl.append(to_add)
                rs.add(to_add)
                prev = to_add
                count2 += 1
            else:
                end = True
        count1 += 1
    return rl


def random_path_pos(to_replace, vertices, edges):  
    rl = []
    tries = 0
    while len(rl) < len(to_replace) and tries < 50:
        found_start = False
        while not found_start:
            start = random.choice(list(vertices))
            if to_replace[0] in vertices[start]:
                rl = [start]
                rs = {start}
                found_start = True   
        prev = start
        for i in range(1, len(to_replace)):
            available = list(connected(prev, edges) - rs)
            if available:
                to_add = False
                random.shuffle(list(available))
                for word in available:
                    try:
                        if to_replace[i] in vertices[word]:
                            to_add = True
                            word_to_add = word
                            break
                    except KeyError:
                        pass
                if to_add:
                    rl.append(word_to_add)
                    rs.add(word_to_add)
                    prev = word_to_add
        tries += 1
    return rl


def split_sentence(sentence):
    to_replace = []
    stripped = []
    tokens = nltk.word_tokenize(sentence)
    pos_tokens = nltk.pos_tag(tokens, tagset='universal')
    pos_tokens = freeze_words(pos_tokens)
    for token in pos_tokens:
        if token[1] == 'NOUN':
            to_replace.append('n')  
            stripped.append('WORD')
        elif token[1] == 'VERB':
            to_replace.append('v') 
            stripped.append('WORD')
        elif token[1] == 'ADV':
            to_replace.append('r')  
            stripped.append('WORD')
        elif token[1] == 'ADJ':
            to_replace.append('a')
            stripped.append('WORD')
        else:
            stripped.append(token[0])
    return to_replace, stripped


def replace(sentence, vertices, edges): 
    to_replace, stripped = split_sentence(sentence)
    new_words = random_path_pos(to_replace, vertices, edges)
    if len(new_words) == len(to_replace):
        new_sentence = []
        for word in stripped:
            if word == 'WORD':
                new_sentence.append(new_words.pop(0))
            else:
                new_sentence.append(word)
        if new_sentence:
            new_sentence = a_agree(new_sentence)
            new_sentence[0] = new_sentence[0].capitalize()
        return (' ').join(new_sentence)
    else:
        return ('')
    

def a_agree(sentence):
    new_sentence = []
    for word in sentence[:-1]:
        if word == 'a' and re.match('^[aeiou]', sentence[sentence.index(word)+1]):
            new_sentence.append('an')
        elif word == 'an' and re.match('^[^aeiou]', sentence[sentence.index(word)+1]):
            new_sentence.append('a')
        else:
            new_sentence.append(word)
    new_sentence.append(sentence[-1])
    return new_sentence


def freeze_words(tokens):
    pos_tokens = []
    for word in tokens:
        if word[0] == 'i':
              pos_tokens.append(('I', 'NO'))
        elif word[0] in ['is', 'am', 'are', 'was', 
                         'were', 'be', 'being', 'been',
                         'do', 'has', 'had', 'have', 'did',
                         'must', 'can', 'could', 'would', 'might',
                         'ought', 'should']:
            pos_tokens.append((word[0], 'NO'))
        else:
            pos_tokens.append(word)
    return pos_tokens
